# import logging
# from typing import AsyncGenerator, Dict, List
# from langchain_groq import ChatGroq
# from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
# from langgraph.prebuilt import create_react_agent
# from app.core.config import settings
# from app.core.registry import ModelRegistry

# logger = logging.getLogger("Orchestrator")

# class OrchestratorAgent:
#     def __init__(self, tools: List):
#         self.tools = tools
#         self.registry = ModelRegistry()
#         promt = settings.profile
#         print(promt)
#         self.system_prompt = """
#         Ø£Ù†Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù…Ù†ØµØ© Ø§Ø­Ø¬Ø²Ù„ÙŠ.
#         Ù…Ù‡Ù…ØªÙƒ: Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ù†ØµØ© (Ø¨Ø­Ø« Ø¹Ù† Ø±Ø­Ù„Ø§ØªØŒ Ù…Ø¯Ù†ØŒ Ø´Ø±ÙƒØ§Ø¡).
#         Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯: 
#         Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø¯ÙˆØ¯:{promt}
#         1. Ø§Ø¨Ø­Ø« Ø¹Ù† city_id Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø±Ø­Ù„Ø§Øª.
#         2. Ù„Ø§ ØªÙØªÙ ÙÙŠ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±.
#         """

#     async def process_request(self, user_input: str) -> AsyncGenerator[Dict, None]:
#         # Ù†Ø¯Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ù‡Ù†Ø§ ÙƒØ±Ø³Ø§Ù„Ø© Ø£ÙˆÙ„Ù‰ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… state_modifier
#         inputs = {
#             "messages": [
#                 SystemMessage(content=self.system_prompt),
#                 HumanMessage(content=user_input)
#             ]
#         }
        
#         models_to_try = self.registry.get_available_models()

#         for model_name in models_to_try:
#             try:
#                 llm = ChatGroq(
#                     model_name=model_name,
#                     api_key=settings.GROQ_API_KEY,
#                     temperature=0
#                 )
                
#                 # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¨Ø£Ø¨Ø³Ø· ØµÙˆØ±Ø© Ù…Ù…ÙƒÙ†Ø© Ù„ØªØ¬Ù†Ø¨ Ø£Ø®Ø·Ø§Ø¡ Arguments
#                 agent = create_react_agent(llm, self.tools)
                
#                 async for event in agent.astream(inputs, stream_mode="values"):
#                     if not event.get("messages"): continue
                    
#                     last_message = event["messages"][-1]
                    
#                     # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø¯ÙˆØ§Øª (Tools)
#                     if hasattr(last_message, "tool_calls") and last_message.tool_calls:
#                         for call in last_message.tool_calls:
#                             yield {
#                                 "type": "status", 
#                                 "payload": f"Ø§Ø³ØªØ®Ø¯Ø§Ù… {model_name}: Ø¬Ø§Ø±ÙŠ ØªÙ†ÙÙŠØ° {call['name']}..."
#                             }
                    
#                     # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
#                     elif isinstance(last_message, AIMessage) and not last_message.tool_calls:
#                         if last_message.content:
#                             yield {"type": "final", "payload": last_message.content}
                
#                 return # ØªÙ… Ø¨Ù†Ø¬Ø§Ø­ØŒ Ø§Ø®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø­Ù„Ù‚Ø©

#             except Exception as e:
#                 error_str = str(e)
#                 logger.error(f"Model {model_name} failed: {error_str}")
#                 self.registry.report_failure(model_name, error_str)
#                 yield {"type": "status", "payload": f"ÙØ´Ù„ {model_name}ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ§Ù„ÙŠ..."}
#                 continue












# import logging
# from typing import AsyncGenerator, Dict, List
# from langchain_groq import ChatGroq
# from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
# from langgraph.prebuilt import create_react_agent
# from app.core.config import settings
# from app.core.registry import ModelRegistry

# logger = logging.getLogger("Orchestrator")

# class OrchestratorAgent:
#     def __init__(self, tools: List):
#         self.tools = tools
#         self.registry = ModelRegistry()
        
#         # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªØ¹Ø±ÙŠÙÙŠ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
#         profile_content = settings.profile
        
#         # Ø¨Ù†Ø§Ø¡ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
#         self.system_prompt = f"""
#         Ø£Ù†Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù…Ù†ØµØ© Ø§Ø­Ø¬Ø²Ù„ÙŠ.
#         Ù…Ù‡Ù…ØªÙƒ: Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ù†ØµØ© (Ø¨Ø­Ø« Ø¹Ù† Ø±Ø­Ù„Ø§ØªØŒ Ù…Ø¯Ù†ØŒ Ø´Ø±ÙƒØ§Ø¡).
        
#         Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø¯ÙˆØ¯:
#         {profile_content}
        
#         Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØµØ§Ø±Ù…Ø©: 

#         1. Ù„Ø§ ØªÙØªÙ ÙÙŠ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø£Ùˆ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª.
#         """

#     async def process_request(self, user_input: str) -> AsyncGenerator[Dict, None]:
#         inputs = {
#             "messages": [
#                 SystemMessage(content=self.system_prompt),
#                 HumanMessage(content=user_input)
#             ]
#         }
        
#         # Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµØ§Ù„Ø­Ø© Ù…Ù† Ø§Ù„Ù€ Registry Ø§Ù„Ù…Ø­Ø¯Ø« Ù„Ø¯ÙŠÙƒ
#         models_to_try = self.registry.get_available_models()

#         for model_name in models_to_try:
#             try:
#                 llm = ChatGroq(
#                     model_name=model_name,
#                     api_key=settings.GROQ_API_KEY,
#                     temperature=0
#                 )
                
#                 # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„
#                 agent = create_react_agent(llm, self.tools)
                
#                 # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°: Ø±ÙØ¹ Ø­Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
#                 config = {"recursion_limit": 50}
                
#                 async for event in agent.astream(inputs, config=config, stream_mode="values"):
#                     if not event.get("messages"): continue
                    
#                     last_message = event["messages"][-1]
                    
#                     # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø¯ÙˆØ§Øª (Tools)
#                     if hasattr(last_message, "tool_calls") and last_message.tool_calls:
#                         for call in last_message.tool_calls:
#                             yield {
#                                 "type": "status", 
#                                 "payload": f"Ø§Ø³ØªØ®Ø¯Ø§Ù… {model_name}: Ø¬Ø§Ø±ÙŠ ØªÙ†ÙÙŠØ° {call['name']}..."
#                             }
                    
#                     # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
#                     elif isinstance(last_message, AIMessage) and not last_message.tool_calls:
#                         if last_message.content:
#                             yield {"type": "final", "payload": last_message.content}
                
#                 return # Ø§Ù„Ø®Ø±ÙˆØ¬ ÙÙŠ Ø­Ø§Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­

#             except Exception as e:
#                 error_str = str(e)
#                 logger.error(f"Model {model_name} failed: {error_str}")
                
#                 # Ø¥Ø¨Ù„Ø§Øº Ø§Ù„Ù€ Registry Ø¨Ø§Ù„ÙØ´Ù„ Ù„ÙŠÙ‚ÙˆÙ… Ø¨Ø­Ø¸Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (Blacklist)
#                 self.registry.report_failure(model_name, error_str)
                
#                 yield {
#                     "type": "status", 
#                     "payload": f"ÙØ´Ù„ {model_name}ØŒ ÙŠØªÙ… Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ§Ù„ÙŠ..."
#                 }
#                 continue




#!==========================================

# import logging
# from typing import AsyncGenerator, Dict, List, Optional
# from threading import Lock

# from langchain_groq import ChatGroq
# from langchain_core.messages import (
#     HumanMessage,
#     SystemMessage,
#     AIMessage,
#     BaseMessage
# )
# from langgraph.prebuilt import create_react_agent

# from app.core.config import settings
# from app.core.registry import ModelRegistry

# logger = logging.getLogger("Orchestrator")

# # =====================================================
# # 1ï¸âƒ£ Neutral Smart Memory (NO hardcoded data)
# # =====================================================

# class SmartMemory:
#     """
#     Memory Ù…Ø­Ø§ÙŠØ¯Ø© ØªÙ…Ø§Ù…Ù‹Ø§:
#     - Ù†Ø§ÙØ°Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ù‚ØµÙŠØ±Ø©
#     - Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ° (Ø¢Ø®Ø± Ø£Ø¯Ø§Ø© Ø§Ø³ØªÙØ®Ø¯Ù…Øª)
#     Ù„Ø§ ØªÙÙ‡Ù… Ù„ØºØ©ØŒ Ù„Ø§ Ø¯ÙˆÙ…ÙŠÙ†ØŒ Ù„Ø§ Ù†ÙŠØ©ØŒ Ù„Ø§ Ù…Ø¹Ø±ÙØ©.
#     """

#     def __init__(self, window_size: int = 20):
#         self.window_size = window_size
#         self.window: List[BaseMessage] = []
#         self.last_tool_used: Optional[str] = None

#     def add_message(self, message: BaseMessage) -> None:
#         self.window.append(message)
#         self.window = self.window[-self.window_size:]

#     def record_tool_use(self, tool_name: str) -> None:
#         self.last_tool_used = tool_name

#     def render_execution_context(self) -> str:
#         if not self.last_tool_used:
#             return ""
#         return (
#             "Execution context:\n"
#             f"- Last tool used: {self.last_tool_used}"
#         )


# # =====================================================
# # 2ï¸âƒ£ Memory Store (Session Isolation)
# # =====================================================

# class MemoryStore:
#     """
#     Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù†:
#     - Ø¹Ø²Ù„ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ÙƒÙ„ session_id
#     - Ù…Ù†Ø¹ Ø£ÙŠ ØªØ³Ø±ÙŠØ¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
#     """

#     def __init__(self):
#         self._store: Dict[str, SmartMemory] = {}
#         self._lock = Lock()

#     def get(self, session_id: str) -> SmartMemory:
#         with self._lock:
#             if session_id not in self._store:
#                 self._store[session_id] = SmartMemory()
#             return self._store[session_id]

#     def delete(self, session_id: str) -> None:
#         with self._lock:
#             self._store.pop(session_id, None)


# # =====================================================
# # 3ï¸âƒ£ Orchestrator Agent
# # =====================================================

# class OrchestratorAgent:
#     def __init__(self, tools: List):
#         self.tools = tools
#         self.registry = ModelRegistry()
#         self.memory_store = MemoryStore()

#         profile_content = settings.profile

#         # âš ï¸ System prompt ÙÙ‚Ø· â€“ Ø¨Ø¯ÙˆÙ† Ø¨ÙŠØ§Ù†Ø§Øª ØµÙ„Ø¨Ø©
#         self.system_prompt = f"""
# You are the official assistant of the platform.

# Your responsibilities:
# - Help users using the available tools when needed.
# - Never invent data.
# - Never assume parameters.
# - Only rely on tool outputs.
# - If information is missing, ask the user clearly.

# Platform profile:
# {profile_content}
# """.strip()

#     # =================================================
#     # 4ï¸âƒ£ Request Processing (Session-aware)
#     # =================================================

#     async def process_request(
#         self,
#         user_input: str,
#         session_id: str
#     ) -> AsyncGenerator[Dict, None]:

#         memory = self.memory_store.get(session_id)

#         # -----------------------------
#         # Build messages dynamically
#         # -----------------------------

#         messages: List[BaseMessage] = [
#             SystemMessage(content=self.system_prompt)
#         ]

#         execution_context = memory.render_execution_context()
#         if execution_context:
#             messages.append(
#                 SystemMessage(content=execution_context)
#             )

#         messages.extend(memory.window)
#         messages.append(HumanMessage(content=user_input))

#         inputs = {"messages": messages}

#         # -----------------------------
#         # Try available models
#         # -----------------------------

#         models_to_try = self.registry.get_available_models()

#         for model_name in models_to_try:
#             try:
#                 llm = ChatGroq(
#                     model_name=model_name,
#                     api_key=settings.GROQ_API_KEY,
#                     temperature=0
#                 )

#                 agent = create_react_agent(
#                     llm,
#                     self.tools
#                 )

#                 config = {
#                     "recursion_limit": 40
#                 }

#                 async for event in agent.astream(
#                     inputs,
#                     config=config,
#                     stream_mode="values"
#                 ):
#                     if not event.get("messages"):
#                         continue

#                     last_message = event["messages"][-1]

#                     # -------------------------
#                     # Tool Calls
#                     # -------------------------
#                     if hasattr(last_message, "tool_calls") and last_message.tool_calls:
#                         for call in last_message.tool_calls:
#                             tool_name = call.get("name")
#                             if tool_name:
#                                 memory.record_tool_use(tool_name)

#                             yield {
#                                 "type": "status",
#                                 "payload": f"Executing tool: {tool_name}"
#                             }

#                     # -------------------------
#                     # Final AI Response
#                     # -------------------------
#                     elif isinstance(last_message, AIMessage):
#                         if last_message.content:
#                             memory.add_message(
#                                 HumanMessage(content=user_input)
#                             )
#                             memory.add_message(
#                                 AIMessage(content=last_message.content)
#                             )

#                             yield {
#                                 "type": "final",
#                                 "payload": last_message.content
#                             }

#                 return  # Ù†Ø¬Ø§Ø­ â†’ Ù„Ø§ Ù†Ø¬Ø±Ù‘Ø¨ Ù…ÙˆØ¯ÙŠÙ„ Ø¢Ø®Ø±

#             except TypeError as e:
#                 # Ø£Ø®Ø·Ø§Ø¡ Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ³Ø¨Ù‘Ø¨ Blacklist
#                 if "create_react_agent" in str(e):
#                     raise e
#                 raise

#             except Exception as e:
#                 error_str = str(e)
#                 logger.error(
#                     f"Model {model_name} failed: {error_str}"
#                 )

#                 self.registry.report_failure(
#                     model_name,
#                     error_str
#                 )

#                 yield {
#                     "type": "status",
#                     "payload": f"Model {model_name} failed, trying next..."
#                 }

#                 continue























##!2===============================================

# import logging
# import time
# from typing import AsyncGenerator, Dict, List, Optional
# from threading import Lock

# from langchain_groq import ChatGroq
# from langchain_core.messages import (
#     HumanMessage,
#     SystemMessage,
#     AIMessage,
#     BaseMessage
# )
# from langgraph.prebuilt import create_react_agent

# from app.core.config import settings
# from app.core.registry import ModelRegistry

# logger = logging.getLogger("Orchestrator")

# # =====================================================
# # Configuration (NO domain knowledge)
# # =====================================================

# SESSION_TTL_SECONDS = 900          # 15 minutes
# MAX_SESSIONS = 5000                # hard cap
# MAX_TOTAL_MESSAGES = 100_000       # global pressure guard


# # =====================================================
# # 1ï¸âƒ£ Neutral Smart Memory
# # =====================================================

# class SmartMemory:
#     """
#     Memory Ù…Ø­Ø§ÙŠØ¯Ø© ØªÙ…Ø§Ù…Ù‹Ø§:
#     - Ù†Ø§ÙØ°Ø© Ù…Ø­Ø§Ø¯Ø«Ø©
#     - Ø­Ø§Ù„Ø© ØªÙ†ÙÙŠØ°
#     - ÙˆÙ‚Øª Ø¢Ø®Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù… (TTL)
#     """

#     def __init__(self, window_size: int = 20):
#         self.window_size = window_size
#         self.window: List[BaseMessage] = []
#         self.last_tool_used: Optional[str] = None
#         self.last_used: float = time.time()

#     def touch(self) -> None:
#         self.last_used = time.time()

#     def add_message(self, message: BaseMessage) -> None:
#         self.touch()
#         self.window.append(message)
#         self.window = self.window[-self.window_size:]

#     def record_tool_use(self, tool_name: str) -> None:
#         self.touch()
#         self.last_tool_used = tool_name

#     def render_execution_context(self) -> str:
#         if not self.last_tool_used:
#             return ""
#         return (
#             "Execution context:\n"
#             f"- Last tool used: {self.last_tool_used}"
#         )


# # =====================================================
# # 2ï¸âƒ£ Memory Store (Isolation + TTL + Pressure Guard)
# # =====================================================

# class MemoryStore:
#     """
#     - Session isolation
#     - TTL cleanup
#     - Memory pressure protection
#     """

#     def __init__(self):
#         self._store: Dict[str, SmartMemory] = {}
#         self._lock = Lock()

#     def _cleanup_expired(self) -> None:
#         now = time.time()
#         expired = [
#             sid for sid, mem in self._store.items()
#             if now - mem.last_used > SESSION_TTL_SECONDS
#         ]
#         for sid in expired:
#             self._store.pop(sid, None)

#     def _enforce_pressure_limits(self) -> None:
#         # Limit sessions
#         if len(self._store) > MAX_SESSIONS:
#             sorted_sessions = sorted(
#                 self._store.items(),
#                 key=lambda item: item[1].last_used
#             )
#             for sid, _ in sorted_sessions[:len(self._store) - MAX_SESSIONS]:
#                 self._store.pop(sid, None)

#         # Limit total messages
#         total_messages = sum(len(mem.window) for mem in self._store.values())
#         if total_messages > MAX_TOTAL_MESSAGES:
#             sorted_sessions = sorted(
#                 self._store.items(),
#                 key=lambda item: item[1].last_used
#             )
#             for sid, mem in sorted_sessions:
#                 mem.window.clear()
#                 if sum(len(m.window) for m in self._store.values()) <= MAX_TOTAL_MESSAGES:
#                     break

#     def get(self, session_id: str) -> SmartMemory:
#         with self._lock:
#             self._cleanup_expired()
#             self._enforce_pressure_limits()

#             if session_id not in self._store:
#                 self._store[session_id] = SmartMemory()

#             memory = self._store[session_id]
#             memory.touch()
#             return memory

#     def delete(self, session_id: str) -> None:
#         with self._lock:
#             self._store.pop(session_id, None)


# # =====================================================
# # 3ï¸âƒ£ Orchestrator Agent
# # =====================================================

# class OrchestratorAgent:
#     def __init__(self, tools: List):
#         self.tools = tools
#         self.registry = ModelRegistry()
#         self.memory_store = MemoryStore()

#         profile_content = settings.profile

#         self.system_prompt = f"""
# You are the official assistant of the platform.

# Your responsibilities:
# - Help users using the available tools when needed.
# - Never invent data.
# - Never assume parameters.
# - Only rely on tool outputs.
# - If information is missing, ask the user clearly.

# Platform profile:
# {profile_content}
# """.strip()

#     # =================================================
#     # 4ï¸âƒ£ Request Processing (Session-aware)
#     # =================================================

#     async def process_request(
#         self,
#         user_input: str,
#         session_id: str,
#         access_token: str | None = None
#     ) -> AsyncGenerator[Dict, None]:

#         memory = self.memory_store.get(session_id)

#         messages: List[BaseMessage] = [
#             SystemMessage(content=self.system_prompt)
#         ]

#         execution_context = memory.render_execution_context()
#         if execution_context:
#             messages.append(SystemMessage(content=execution_context))

#         messages.extend(memory.window)
#         messages.append(HumanMessage(content=user_input))

#         inputs = {"messages": messages}

#         models_to_try = self.registry.get_available_models()

#         for model_name in models_to_try:
#             try:
#                 llm = ChatGroq(
#                     model_name=model_name,
#                     api_key=settings.GROQ_API_KEY,
#                     temperature=0
#                 )

#                 agent = create_react_agent(
#                     llm,
#                     self.tools
#                 )

#                 config = {"recursion_limit": 40}

#                 async for event in agent.astream(
#                     inputs,
#                     config=config,
#                     stream_mode="values"
#                 ):
#                     if not event.get("messages"):
#                         continue

#                     last_message = event["messages"][-1]

#                     # Tool calls
#                     if hasattr(last_message, "tool_calls") and last_message.tool_calls:
#                         for call in last_message.tool_calls:
#                             tool_name = call.get("name")
#                             if tool_name:
#                                 memory.record_tool_use(tool_name)

#                             yield {
#                                 "type": "status",
#                                 "payload": f"Executing tool: {tool_name}"
#                             }

#                     # Final response
#                     elif isinstance(last_message, AIMessage):
#                         if last_message.content:
#                             memory.add_message(
#                                 HumanMessage(content=user_input)
#                             )
#                             memory.add_message(
#                                 AIMessage(content=last_message.content)
#                             )

#                             yield {
#                                 "type": "final",
#                                 "payload": last_message.content
#                             }

#                 return

#             except TypeError as e:
#                 # Ø£Ø®Ø·Ø§Ø¡ Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ø§ ÙŠØ¬Ø¨ Ø£Ù† ØªØ³Ø¨Ù‘Ø¨ blacklist
#                 if "create_react_agent" in str(e):
#                     raise e
#                 raise

#             except Exception as e:
#                 error_str = str(e)
#                 logger.error(f"Model {model_name} failed: {error_str}")

#                 self.registry.report_failure(model_name, error_str)

#                 yield {
#                     "type": "status",
#                     "payload": f"Model {model_name} failed, trying next..."
#                 }

#!=================
# #app/agents/orchestrator.py

# import logging
# import time
# from typing import AsyncGenerator, Dict, List, Optional
# from threading import Lock

# from langchain_groq import ChatGroq
# from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
# from langgraph.prebuilt import create_react_agent

# from app.core.config import settings
# from app.core.registry import ModelRegistry
# from app.services.api_service import ApiService
# from app.core.execution_context import current_execution_context



# logger = logging.getLogger("OrchestratorFullDebug")

# SESSION_TTL_SECONDS = 900
# MAX_SESSIONS = 5000
# MAX_TOTAL_MESSAGES = 100_000

# class SmartMemory:
#     def __init__(self, window_size: int = 10):
#         self.window_size = window_size
#         self.window: List[BaseMessage] = []
#         self.last_tool_used: Optional[str] = None
#         self.last_used: float = time.time()

#     def touch(self):
#         self.last_used = time.time()

#     def add_message(self, message: BaseMessage):
#         self.touch()
#         self.window.append(message)
#         self.window = self.window[-self.window_size:]

#     def record_tool_use(self, tool_name: str):
#         self.touch()
#         self.last_tool_used = tool_name

#     def render_execution_context(self) -> str:
#         if not self.last_tool_used:
#             return ""
#         return f"Execution context:\n- Last tool used: {self.last_tool_used}"

# class MemoryStore:
#     def __init__(self):
#         self._store: Dict[str, SmartMemory] = {}
#         self._lock = Lock()

#     def _cleanup_expired(self):
#         now = time.time()
#         expired = [sid for sid, mem in self._store.items() if now - mem.last_used > SESSION_TTL_SECONDS]
#         for sid in expired:
#             self._store.pop(sid, None)

#     def _enforce_pressure_limits(self):
#         if len(self._store) > MAX_SESSIONS:
#             sorted_sessions = sorted(self._store.items(), key=lambda item: item[1].last_used)
#             for sid, _ in sorted_sessions[:len(self._store) - MAX_SESSIONS]:
#                 self._store.pop(sid, None)
#         total_messages = sum(len(mem.window) for mem in self._store.values())
#         if total_messages > MAX_TOTAL_MESSAGES:
#             sorted_sessions = sorted(self._store.items(), key=lambda item: item[1].last_used)
#             for _, mem in sorted_sessions:
#                 mem.window.clear()
#                 if sum(len(m.window) for m in self._store.values()) <= MAX_TOTAL_MESSAGES:
#                     break

#     def get(self, session_id: str) -> SmartMemory:
#         with self._lock:
#             self._cleanup_expired()
#             self._enforce_pressure_limits()
#             if session_id not in self._store:
#                 self._store[session_id] = SmartMemory()
#             memory = self._store[session_id]
#             memory.touch()
#             return memory

# class OrchestratorAgent:
#     def __init__(self, tools: List):
#         self.tools = tools
#         self.registry = ModelRegistry()
#         self.memory_store = MemoryStore()
#         profile_content = settings.profile
#         docs_info = settings.api_docs
#         self.system_prompt = f"""
# You are the official assistant of the platform.

# Rules:
# - Use tools only when needed.
# - Never invent data.
# - Never assume parameters.
# - Only rely on tool outputs.
# - If information is missing, ask the user clearly.

# Platform profile:
# {profile_content}

# Available tools and their documentation:
# {docs_info}
# """.strip()
    
#     async def process_request(self, user_input: str, session_id: str, access_token: Optional[str] = None) -> AsyncGenerator[Dict, None]:
#         memory = self.memory_store.get(session_id)
#         current_execution_context.set({
#     "session_id": session_id,
#     "access_token": access_token
#         })

#         print("\n=== DEBUG EXECUTION CONTEXT ===")
#         print(current_execution_context.get())

#         messages: List[BaseMessage] = [SystemMessage(content=self.system_prompt)]
#         execution_hint = memory.render_execution_context()
#         if execution_hint:
#             messages.append(SystemMessage(content=execution_hint))
#         messages.extend(memory.window)
#         messages.append(HumanMessage(content=user_input))

#         inputs = {"messages": messages, "execution_context": current_execution_context.get()}

#         for model_name in self.registry.get_available_models():
#             try:
#                 llm = ChatGroq(model_name=model_name, api_key=settings.GROQ_API_KEY, temperature=0)
#                 agent = create_react_agent(llm, self.tools)
#                 async for event in agent.astream(inputs, config={"recursion_limit":40}, stream_mode="values"):
#                     if not event.get("messages"):
#                         continue
#                     last_message = event["messages"][-1]
                    

#                     if hasattr(last_message, "tool_calls") and last_message.tool_calls:
#                         for call in last_message.tool_calls:
#                             tool_name = call.get("name")
#                             if tool_name:
#                                 memory.record_tool_use(tool_name)
#                             print(f"\n=== DEBUG TOOL CALL START ===\nTool: {tool_name}\nExecution Context: {current_execution_context.get()}\n")
#                             yield {"type": "status", "payload": f"Executing tool: {tool_name}"}
#                     elif isinstance(last_message, AIMessage):
#                         if last_message.content:
#                             memory.add_message(HumanMessage(content=user_input))
#                             memory.add_message(AIMessage(content=last_message.content))
#                             print(f"\n=== DEBUG AI RESPONSE ===\n{last_message.content}\n")
#                             yield {"type": "final", "payload": last_message.content}
#                 return
#             except Exception as e:
#                 logger.error(f"Model {model_name} failed: {e}")
#                 yield {"type": "status", "payload": f"Model {model_name} failed: {e}"}


# #!!============
# import logging
# import json
# from typing import AsyncGenerator, Dict, List, Optional

# from langchain_groq import ChatGroq
# from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
# from langgraph.prebuilt import create_react_agent

# from app.core.config import settings
# from app.core.registry import ModelRegistry
# from app.core.execution_context import current_execution_context

# # 1. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ© + Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª)
# from app.core.memory import memory_engine  # (RAG Memory)
# from app.core.memory.user_profile_db import UserProfileManager  # <--- (NEW) Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰

# logger = logging.getLogger("OrchestratorAgent")

# class OrchestratorAgent:
#     def __init__(self, tools: List):
#         self.tools = tools
#         self.registry = ModelRegistry()
        
#         # 2. ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
#         self.profile_db = UserProfileManager()  # <--- (NEW)
        
#         profile_content = settings.profile
        
#         # System Prompt Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
#         self.base_system_prompt = f"""
# You are the official assistant of the platform.
# Rules:
# - Use tools only when needed.
# - Never invent data.
# - Only rely on tool outputs or the provided CONTEXT below.
# - If information is missing, ask the user clearly.
# Platform profile:
# {profile_content}
# """.strip()
    
#     async def process_request(self, user_input: str, session_id: str, access_token: Optional[str] = None) -> AsyncGenerator[Dict, None]:
#         # ØªØ­Ø¯ÙŠØ¯ Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙˆØ­Ø¯
#         user_key = session_id if session_id else f"access_token:{access_token}"
        
#         current_execution_context.set({
#             "session_id": session_id,
#             "access_token": access_token,
#             "user_id": user_key
#         })

#         # ============================================================
#         # Ø®Ø·ÙˆØ© 1: Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ (UserProfileDB)
#         # ============================================================
#         # <--- (NEW BLOCK)
#         user_profile = self.profile_db.get_profile(user_key)
#         preferences = user_profile.get("preferences", {})
        
#         # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª ÙƒÙ†Øµ Ù„ÙŠÙ‚Ø±Ø£Ù‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
#         preferences_context = ""
#         if preferences:
#             preferences_list = [f"- {k}: {v}" for k, v in preferences.items()]
#             preferences_context = "\n".join(preferences_list)
#         # ============================================================

#         # Ø®Ø·ÙˆØ© 2: Ø¨Ù†Ø§Ø¡ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ… (RAG Memory)
#         memory_context = memory_engine.build_context(user_key, user_input)

#         # Ø®Ø·ÙˆØ© 3: Ø¯Ù…Ø¬ ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„Ù€ System Prompt
#         enriched_system_prompt = self.base_system_prompt
        
#         # Ø£. Ø¥Ø¶Ø§ÙØ© ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰)
#         if preferences_context:
#              # <--- (NEW) Ø¥Ø®Ø¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
#             enriched_system_prompt += f"\n\n### KNOWN USER PREFERENCES (Do not ask about these again):\n{preferences_context}"
        
#         # Ø¨. Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
#         if memory_context.get("summary"):
#             enriched_system_prompt += f"\n\n### CONVERSATION SUMMARY:\n{memory_context['summary']}"
        
#         # Ø¬. Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
#         if memory_context.get("relevant_memories"):
#             memories_text = "\n".join([json.dumps(m, ensure_ascii=False) for m in memory_context['relevant_memories']])
#             enriched_system_prompt += f"\n\n### RELEVANT HISTORY:\n{memories_text}"

#         # Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
#         messages: List[BaseMessage] = [SystemMessage(content=enriched_system_prompt)]

#         # Ø¥Ø¶Ø§ÙØ© Ø¢Ø®Ø± Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ (Recent History)
#         for text in memory_context.get("recent_messages", []):
#             messages.append(HumanMessage(content=f"[History]: {text}"))

#         messages.append(HumanMessage(content=user_input))

#         # Ø­ÙØ¸ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
#         memory_engine.ingest_text(user_key, f"User: {user_input}")

#         inputs = {"messages": messages}

#         # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Loop through models)
#         for model_name in self.registry.get_available_models():
#             try:
#                 llm = ChatGroq(model_name=model_name, api_key=settings.GROQ_API_KEY, temperature=0)
#                 agent = create_react_agent(llm, self.tools)
                
#                 final_response = ""

#                 async for event in agent.astream(inputs, config={"recursion_limit": 15}, stream_mode="values"):
#                     if not event.get("messages"): continue
#                     last_message = event["messages"][-1]

#                     if hasattr(last_message, "tool_calls") and last_message.tool_calls:
#                         for call in last_message.tool_calls:
#                             yield {"type": "status", "payload": f"Using tool: {call.get('name')}"}

#                     elif isinstance(last_message, AIMessage):
#                         if last_message.content:
#                             final_response = last_message.content
#                             yield {"type": "final", "payload": final_response}
                
#                 # Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
#                 if final_response:
#                     memory_engine.ingest_text(user_key, f"AI: {final_response}")
                
#                 return

#             except Exception as e:
#                 logger.error(f"Model {model_name} failed: {e}")
#                 self.registry.report_failure(model_name, str(e))
#                 yield {"type": "status", "payload": f"Error with {model_name}, switching..."}











#!!!!=========

import logging
import json
from typing import AsyncGenerator, Dict, List, Optional, Any

from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langgraph.prebuilt import create_react_agent

from app.core.config import settings
from app.core.registry import ModelRegistry
from app.core.execution_context import current_execution_context

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø·ÙˆÙŠÙ„Ø©)
from app.core.memory import memory_engine  # (RAG - Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©)
from app.core.memory.user_profile_db import UserProfileManager  # (Profile DB - Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¬Ù„ (Logger) Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„Ø£Ø­Ø¯Ø§Ø«
logger = logging.getLogger("OrchestratorAgent")

class OrchestratorAgent:
    """
    Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ù…Ù†Ø³Ù‚ (Orchestrator): Ù‡Ùˆ Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…Ø¯Ø¨Ø± Ù„Ù„Ù†Ø¸Ø§Ù….
    Ù…Ø³Ø¤ÙˆÙ„ÙŠØªÙ‡: Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø¯ÙˆØ§ØªØŒ Ø§Ø³ØªØ­Ø¶Ø§Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ØŒ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­ÙˆØ§Ø±.
    """
    
    def __init__(self, tools: List[Any]):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ù†Ø³Ù‚.
        :param tools: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠÙØ³Ù…Ø­ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§.
        """
        self.tools = tools
        self.registry = ModelRegistry()  # Ø³Ø¬Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ù„Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„)
        self.profile_db = UserProfileManager()  # Ù…Ø¯ÙŠØ± Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª
        
        # Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (System Prompt): Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø«Ø§Ø¨ØªØ© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØªØºÙŠØ±
        # Ù†ÙƒØªØ¨Ù‡Ø§ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„Ø£Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªÙÙ‡Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¨Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰
        self.base_system_prompt = f"""
You are the official AI assistant of the platform.

### CORE OPERATING RULES:
1. **MEMORY & PERSONALIZATION**:
   - If the user mentions a personal preference (e.g., "I prefer window seats", "I pay cash"), use the 'save_user_preference' tool IMMEDIATELY.
   - Do NOT ask for permission to save preferences. Act proactively.
   
2. **TOOL USAGE**:
   - Use tools ONLY when necessary. Do not guess information.
   - If inputs are missing, ask the user for clarification.

### PLATFORM PROFILE:
{settings.profile}
""".strip()

    def _build_enhanced_system_prompt(self, user_key: str, memory_context: Dict, user_input: str) -> str:
        """
        Ø¯Ø§Ù„Ø© Ø¯Ø§Ø®Ù„ÙŠØ© Ù…Ø³Ø¤ÙˆÙ„Ø© ÙÙ‚Ø· Ø¹Ù† Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Prompt Engineering).
        ØªÙ‚ÙˆÙ… Ø¨Ø¯Ù…Ø¬ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª + Ø§Ù„Ø°Ø§ÙƒØ±Ø© + Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙÙŠ Ù†Øµ ÙˆØ§Ø­Ø¯.
        """
        # 1. Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰)
        user_profile = self.profile_db.get_profile(user_key)
        preferences = user_profile.get("preferences", {})
        
        system_prompt = self.base_system_prompt

        # 2. Ø­Ù‚Ù† Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª (Ø¥Ù† ÙˆØ¬Ø¯Øª)
        if preferences:
            pref_list = [f"- {k}: {v}" for k, v in preferences.items()]
            pref_text = "\n".join(pref_list)
            system_prompt += f"\n\n### ğŸ‘¤ KNOWN USER PREFERENCES (Consider these implicitly):\n{pref_text}"

        # 3. Ø­Ù‚Ù† Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        if memory_context.get("summary"):
            system_prompt += f"\n\n### ğŸ“ CONVERSATION SUMMARY:\n{memory_context['summary']}"

        # 4. Ø­Ù‚Ù† Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© (RAG Context)
        if memory_context.get("relevant_memories"):
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø¥Ù„Ù‰ Ù†Øµ JSON Ù…Ø¶ØºÙˆØ·
            memories_text = "\n".join([json.dumps(m, ensure_ascii=False) for m in memory_context['relevant_memories']])
            system_prompt += f"\n\n### ğŸ§  RELEVANT MEMORY & HISTORY:\n{memories_text}"
            
        # 5. Ø¥Ø¶Ø§ÙØ© ØªØ°ÙƒÙŠØ± Ø¨Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ (Ù„Ø¶Ù…Ø§Ù† Ø¹Ù…Ù„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­)
        system_prompt += f"\n\n### CURRENT CONTEXT:\nUser ID: {user_key}"

        return system_prompt

    async def process_request(self, user_input: str, session_id: str, access_token: Optional[str] = None) -> AsyncGenerator[Dict, None]:
        """
        Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø·Ù„Ø¨.
        ÙŠÙ‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ -> Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø°Ø§ÙƒØ±Ø© -> ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.
        """
        if not session_id or session_id == "guest":
        # Ø­Ø§Ù„Ø© Ø·Ø§Ø±Ø¦Ø©: Ù„Ø§ ØªÙˆÙƒÙ† ÙˆÙ„Ø§ Ø±Ù‚Ù… Ø¬Ù„Ø³Ø©
            # Ù†ÙˆÙ„Ø¯ Ù…Ø¹Ø±Ù Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ø­Ø¸ÙŠ (Ù„Ù† ÙŠÙØ­ÙØ¸ Ø¨Ø¹Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ø·Ù„Ø¨)
            import uuid
            session_id = str(uuid.uuid4())
        # 1. ØªØ­Ø¯ÙŠØ¯ Ù…ÙØªØ§Ø­ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (User Key) Ø¨Ø´ÙƒÙ„ Ù…ÙˆØ­Ø¯
        user_key = session_id if session_id else f"access_token:{access_token}"
        
        # Ø­ÙØ¸ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù† ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ (Global Context)
        current_execution_context.set({
            "session_id": session_id,
            "access_token": access_token,
            "user_id": user_key
        })

        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_key}")

        # 2. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø°ÙƒÙŠ (RAG Memory Lookup)
        # Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ© ØªØ¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ Ù…ØªØ¹Ù„Ù‚ Ø¨Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ
        memory_context = memory_engine.build_context(user_key, user_input)

        # 3. Ø¨Ù†Ø§Ø¡ "Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù…Ø­Ø³Ù†" (The Enhanced Prompt)
        final_system_prompt = self._build_enhanced_system_prompt(user_key, memory_context, user_input)

        # 4. ØªØ¬Ù‡ÙŠØ² Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        messages: List[BaseMessage] = [SystemMessage(content=final_system_prompt)]
        
        # Ø¥Ø¶Ø§ÙØ© Ø¢Ø®Ø± Ø¨Ø¶Ø¹ Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø±ÙŠØ¨
        for text in memory_context.get("recent_messages", []):
            messages.append(HumanMessage(content=f"[History]: {text}"))
        
        # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        messages.append(HumanMessage(content=user_input))

        # 5. ØªØ³Ø¬ÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„)
        memory_engine.ingest_text(user_key, f"User: {user_input}")

        inputs = {"messages": messages}

        # 6. Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ù…Ø¹ "Ø¢Ù„ÙŠØ© Ø§Ù„ØªØ¹Ø§ÙÙŠ Ù…Ù† Ø§Ù„ÙØ´Ù„" (Fallback Mechanism)
        # Ù†Ø­Ø§ÙˆÙ„ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆÙ„ØŒ Ø¥Ø°Ø§ ÙØ´Ù„ Ù†Ù†ØªÙ‚Ù„ Ù„Ù„Ø«Ø§Ù†ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
        available_models = self.registry.get_available_models()
        
        for model_name in available_models:
            try:
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                llm = ChatGroq(
                    model_name=model_name, 
                    api_key=settings.GROQ_API_KEY, 
                    temperature=0.0  # ØµÙØ± Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ¹Ø¯Ù… Ø§Ù„Ù‡Ù„ÙˆØ³Ø©
                )
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„ (ReAct Agent)
                agent = create_react_agent(llm, self.tools)
                
                final_response = ""

                # Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø« (Streaming)
                async for event in agent.astream(inputs, config={"recursion_limit": 15}, stream_mode="values"):
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø±Ø³Ø§Ø¦Ù„
                    if not event.get("messages"): continue
                    last_message = event["messages"][-1]

                    # Ø§Ù„Ø­Ø§Ù„Ø© Ø£: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø©
                    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                        for call in last_message.tool_calls:
                            tool_name = call.get('name')
                            logger.info(f"ğŸ› ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¯Ø§Ø©: {tool_name}")
                            yield {"type": "status", "payload": f"Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯Ø§Ø©: {tool_name}..."}

                    # Ø§Ù„Ø­Ø§Ù„Ø© Ø¨: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ø¹Ø·Ù‰ Ø±Ø¯Ø§Ù‹ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹
                    elif isinstance(last_message, AIMessage):
                        if last_message.content:
                            final_response = last_message.content
                            yield {"type": "final", "payload": final_response}
                
                # Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù‡Ù†Ø§ØŒ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ØªÙ…Øª Ø¨Ù†Ø¬Ø§Ø­
                # Ø­ÙØ¸ Ø±Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                if final_response:
                    memory_engine.ingest_text(user_key, f"AI: {final_response}")
                
                return  # Ø®Ø±ÙˆØ¬ Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø© (Ù„Ø§ Ø¯Ø§Ø¹ÙŠ Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ø±Ù‰)

            except Exception as e:
                # ÙÙŠ Ø­Ø§Ù„ Ø­Ø¯ÙˆØ« Ø®Ø·Ø£ØŒ Ù†Ø³Ø¬Ù„Ù‡ ÙˆÙ†Ø­Ø§ÙˆÙ„ Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ§Ù„ÙŠ
                logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}: {e}")
                self.registry.report_failure(model_name, str(e))
                yield {"type": "status", "payload": f"ÙˆØ§Ø¬Ù‡Ù†Ø§ Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ {model_name}ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ù„Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ..."}

        # Ø¥Ø°Ø§ ÙØ´Ù„Øª ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ù†Ø§Ø¯Ø± Ø§Ù„Ø­Ø¯ÙˆØ«)
        yield {"type": "error", "payload": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø´ØºÙˆÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹."}